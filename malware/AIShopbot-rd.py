import base64
import socket
import ssl
import string
import sys
import asyncio
import time
import aiohttp
import httpx
import aiofiles
import random
import json
import csv
from datetime import datetime, timedelta
from collections import deque, defaultdict
import math
import os


import random
import uuid
from datetime import datetime, timedelta
import json
import urllib.parse

from PyQt6.QtWidgets import (QApplication, QMainWindow, QVBoxLayout, QHBoxLayout, QWidget, QPushButton, 
                             QProgressBar, QFileDialog, QTextEdit, QLabel, QLineEdit, QScrollArea, QFrame,
                             QSpinBox, QMessageBox)
from PyQt6.QtCore import QThread, pyqtSignal, Qt, QTimer
from openai import AsyncOpenAI, OpenAI
from variables import first_names, last_names
from collections import Counter

import json
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse
from collections import OrderedDict


class MessageGenerator:
    def __init__(self):
        self.client = AsyncOpenAI(api_key='sk-proj-redactedofc')

    async def extract_relevant_text(self, html_content):
        soup = BeautifulSoup(html_content, 'html.parser')
        for script in soup(["script", "style"]):
            script.decompose()
        
        relevant_tags = ['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p', 'a', 'span']
        extracted_text = OrderedDict()
        
        for tag in relevant_tags:
            elements = soup.find_all(tag)
            for element in elements:
                text = element.get_text(strip=True)
                if text and len(text) > 3:
                    if text not in extracted_text:
                        extracted_text[text] = tag
        
        organized_text = {tag: [] for tag in relevant_tags}
        for text, tag in extracted_text.items():
            organized_text[tag].append(text)
        
        final_text = []
        for tag in relevant_tags:
            if organized_text[tag]:
                final_text.append(f"{tag.upper()}:")
                final_text.extend(organized_text[tag])
                final_text.append("")
        
        return "\n".join(final_text)

    async def generate_email_template(self, context, url):
        with open('templates.txt', 'r') as f:
            templates = f.read()
        prompt = f"""
Based on the following context from a store's website ({url}), create a marketing message using this template:

{templates}

Website context:
{context}

Instructions:
1. Replace <WEBSITE_NAME> with the actual name of the website if you can find it in the context. If not, use the domain name without 'www' or '.com'.

2. For <SPECIALIZATION_CONTENT>:
   - If the website appears to be a template or placeholder (e.g., containing "Example product title" or similar generic content), use:
     "I noticed your website is currently in development. This is an excellent time to start thinking about your online visibility."
   - Otherwise, use:
     "I see that your website specializes in <KEYWORD>."
     Replace <KEYWORD> with a brief, inclusive description of the main products or services offered by the website. If unclear, use "various products and services".

3. For <CUSTOMER_INTEREST>:
   - If the website appears to be a template or placeholder (e.g., containing "Example product title" or similar generic content), use default message
   - Otherwise, use: "that are interested in <KEYWORD>"

4. For <CUSTOM_CONTENT_BASED_ON_WEBSITE>, if we're not using default message, add 1-2 sentences that:
    - Highlight unique selling points or a diverse range of products/services offered by the website.
    - Avoid subjective terms like "whimsical," "cute," or "adorable" unless explicitly used by the website itself.
    - Include a brief statement about how increased online visibility can benefit the business.

5. If there's very little specific information available about the website's products or services (but it's not a template), use this default message:
   
6. Ensure the message sounds personalized and relevant to the specific website, while remaining professional and avoiding overly casual language.

7. Do not modify or remove {{first_name}} or {{random_string}}. These are placeholders and should remain exactly as they are.

Remember: Create an accurate, inclusive, and effective marketing message that reflects the actual state of the website, be general and not specific when talking about their products or services. don't use "particularly" or "specifically" and keep it general. Emphasize how our services can increase their online visibility and customer base, tailoring the message to their current situation.
"""

        try:
            response = await self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "You are a helpful assistant that generates marketing messages."},
                    {"role": "user", "content": prompt}
                ]
            )
            return response.choices[0].message.content
        except Exception as e:
            print(f"Error generating email template for {url}: {str(e)}")
            return ""

    def normalize_url(self, url):
        parsed = urlparse(url)
        return f"{parsed.scheme}://{parsed.netloc}" if parsed.scheme else f"https://{parsed.netloc or parsed.path}"

    async def generate_message(self, first_name, url):
        normalized_url = self.normalize_url(url)
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(normalized_url, timeout=30) as response:
                    response.raise_for_status()
                    html_content = await response.text()
            
            relevant_text = await self.extract_relevant_text(html_content)
            if len(relevant_text) > 15000:
                relevant_text = relevant_text[:15000] + "..."
            
            email_template = await self.generate_email_template(relevant_text, normalized_url)
            
            # Replace placeholders
            message = email_template.format(first_name=first_name, random_string=self.generate_random_string())
            return message
        except Exception as e:
            print(f"Error processing {url}: {str(e)}")
            return ""

    def generate_random_string(self):
        return ''.join(random.choices(string.ascii_uppercase + string.digits, k=5))

class BandwidthTracker:
    def __init__(self):
        self.total_bytes_sent = 0
        self.total_bytes_received = 0
        self.start_time = time.time()

    def update(self, bytes_sent, bytes_received):
        self.total_bytes_sent += bytes_sent
        self.total_bytes_received += bytes_received

    def get_usage(self):
        duration = time.time() - self.start_time
        total_bytes = self.total_bytes_sent + self.total_bytes_received
        mbps = (total_bytes * 8 / 1_000_000) / duration
        return {
            'sent_mb': self.total_bytes_sent / 1_000_000,
            'received_mb': self.total_bytes_received / 1_000_000,
            'total_mb': total_bytes / 1_000_000,
            'duration_seconds': duration,
            'mbps': mbps
        }

class URLList:
    def __init__(self, filename, urls):
        self.filename = filename
        self.urls = urls
        self.length = len(urls)
        self.urls_per_minute = 0

class AsyncWorker(QThread):
    progress_update = pyqtSignal(int, int, int)
    status_update = pyqtSignal(str, str)
    speed_update = pyqtSignal(float)
    success_rate_update = pyqtSignal(float)
    stats_update = pyqtSignal(int, int, int, int)
    thread_stats_update = pyqtSignal(int, int)
    bandwidth_update = pyqtSignal(dict)

    def __init__(self, url_lists, proxy, thread_count):
        super().__init__()
        self.url_lists = url_lists
        self.proxy = proxy
        self.thread_count = thread_count
        self.is_running = False
        self.is_paused = False
        self.processed_urls = 0
        self.total_urls = sum(url_list.length for url_list in url_lists)
        self.successful_submissions = 0
        self.captcha_required = 0
        self.pages_not_found = 0
        self.unknown_cases = 0
        self.submission_history = deque(maxlen=300)
        self.last_minute_submissions = deque(maxlen=60)
        self.last_minute_token_fetches = deque(maxlen=60)
        self.start_time = None
        self.stop_event = asyncio.Event()
        self.tokens_received = deque(maxlen=6000)
        self.submissions_performed = deque(maxlen=6000)
        self.bandwidth_tracker = BandwidthTracker()
        self.max_retries = 3
        self.urls = [(url, url_list.filename) for url_list in url_lists for url in url_list.urls]
        self.current_url_index = 0
        self.url_lock = asyncio.Lock()
        self.url_queue = asyncio.Queue()  # Add this line to create the url_queue
        self.stats_task = None


    async def run_async(self):
        self.start_time = datetime.now()
        self.is_running = True
        
        # Start the stats update task
        self.stats_task = asyncio.create_task(self.update_stats_periodically())
        
        tasks = [self.process_url() for _ in range(self.thread_count)]
        await asyncio.gather(*tasks)


    async def update_stats_periodically(self):
        while self.is_running:
            self.update_json_stats()
            await asyncio.sleep(1)  # Update every 60 seconds

    def run(self):
        asyncio.run(self.run_async())

    async def process_url(self):
        while not self.stop_event.is_set():
            if self.is_paused:
                await asyncio.sleep(0.1)
                continue

            try:
                _, (url, filename) = await self.url_queue.get()
            except asyncio.QueueEmpty:
                break

            self.thread_stats_update.emit(self.thread_count, self.url_queue.qsize())

            connection_retry_count = 0
            processed = False

            while not processed:
                try:
                    placeholder_url = 'https://' + url.split('//')[-1].split('/')[0]
                    
                    while True:
                        data = await self.fetch_token()
                        if 'hcaptcha' in data and 'form_key' in data:
                            break
                        await asyncio.sleep(2)  # Silent wait for token

                    hcaptcha = data["hcaptcha"]
                    formkey = data["form_key"]
                    
                    self.tokens_received.append(datetime.now())
                    
                    first_name = random.choice(first_names)
                    last_name = random.choice(last_names)
                    email = self.generate_email(first_name, last_name)
                    first_name = first_name[0].upper() + first_name[1:]
                    message = await self.generate_message(first_name, filename, placeholder_url)
                    if not message:
                        self.status_update.emit("WARNING", f"Failed to generate message for {url}")
                        template = self.select_template(filename)
                        message = template.format(first_name=first_name, random_string=self.generate_random_string())
                    message = message[0].upper() + message[1:]
                    status, headers, response, proxy = await self.make_request(placeholder_url, hcaptcha, formkey, first_name, email, message)
                    
                    
                    location = headers.get('Location', '') or headers.get('location', '')
                    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')

                    if 'true' in location.lower():
                        self.successful_submissions += 1
                        self.status_update.emit("INFO", f"Submitted successfully: {url} | {proxy} | message: {message}")
                        self.write_to_csv(timestamp, url, filename, f'Success proxy: {proxy}', message)
                        processed = True
                        self.submissions_performed.append(datetime.now())
                    elif status == 400 or 'challenge' in location.lower():
                        if 'challenge' in location.lower():
                            refresh = json.load(open('captcha-solver.json')).get("refresh", "false")
                            if refresh == "false": 
                                json.dump({"refresh": "true"}, open('captcha-solver.json', 'w'))
                            self.captcha_required += 1
                            self.status_update.emit("WARNING", f"Captcha required: {url} | {proxy}")
                            self.write_to_csv(timestamp, url, filename, f'Captcha Required proxy: {proxy}', message)
                        else:
                            self.status_update.emit("WARNING", f"400 error, possibly invalid captcha: {url} | {proxy}")
                        # Continue the loop for infinite captcha/400 retries
                        await asyncio.sleep(1)  # Wait before retrying
                    elif status == 0:  # Connection error
                        connection_retry_count += 1
                        if connection_retry_count < self.max_retries:
                            self.status_update.emit("WARNING", f"Connection error: {url} | Status: {status} | Proxy: {proxy} | Retry: {connection_retry_count}")
                            await asyncio.sleep(1)  # Wait before retrying
                        else:
                            self.status_update.emit("ERROR", f"Max retries reached for connection error: {url}")
                            processed = True  # Proceed after max connection retries
                    else:  # Unknown case or weird status code
                        self.status_update.emit("WARNING", f"Unknown case: {url} | Status: {status}")
                        self.write_to_csv(timestamp, url, filename, f'Unknown case: Status {status}, Headers: {headers} | {proxy}', message)
                        self.unknown_cases += 1
                        processed = True  # Proceed for unknown cases

                except Exception as e:
                    connection_retry_count += 1
                    if connection_retry_count < self.max_retries:
                        self.status_update.emit("ERROR", f"Error processing URL {url}: {str(e)} | Retry: {connection_retry_count}")
                        await asyncio.sleep(1)
                    else:
                        self.status_update.emit("ERROR", f"Max retries reached for connection error: {url}")
                        processed = True  # Proceed after max connection retries

            self.processed_urls += 1
            self.progress_update.emit(self.processed_urls, self.total_urls, int(self.processed_urls / self.total_urls * 100))
            self.stats_update.emit(self.captcha_required, self.successful_submissions, self.pages_not_found, self.unknown_cases)
            self.update_speed()  # Add this line
            self.update_success_rate()

            async with self.url_lock:
                self.current_url_index += 1

        if self.processed_urls >= self.total_urls:
            self.stop()
        
    async def fetch_token(self):
        max_retries = 3
        for _ in range(max_retries):
            try:
                url = 'http://localhost:9999/data'
                async with aiohttp.ClientSession() as session:
                    async with session.get(url, ssl=False) as response:
                        return await response.json()
            except Exception as e:
                self.status_update.emit("ERROR", f"Error fetching token: {str(e)}")
                await asyncio.sleep(1)
        return {}

    # async def make_request(self, placeholder_url, hcaptcha, formkey, name, email, message):
    #     ips = open('proxies.txt', 'r+').read()
    #     ips = [ip.strip() for ip in ips.split('\n') if ip.strip()]
    #     proxy = random.choice(ips).split(':')
    #     if len(proxy) == 4:
    #         proxy_url = f"http://{proxy[2]}:{proxy[3]}@{proxy[0]}:{proxy[1]}"
    #     elif len(proxy) == 2:
    #         proxy_url = f"http://{proxy[0]}:{proxy[1]}"
    #     else:
    #         self.status_update.emit("ERROR", f"Invalid proxy format: {':'.join(proxy)}")
    #         return 502, {}, "", ""
    #     headers = {
    #         'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
    #         'cache-control': 'max-age=0',
    #         'content-type': 'application/x-www-form-urlencoded',
    #         'origin': placeholder_url,
    #         'priority': 'u=0, i',
    #         'referer': f'{placeholder_url}/pages/contact',
    #         'sec-ch-ua': '"Not)A;Brand";v="99", "Google Chrome";v="127", "Chromium";v="127"',
    #         'sec-ch-ua-mobile': '?0',
    #         'sec-ch-ua-platform': '"Windows"',
    #         'sec-fetch-dest': 'document',
    #         'sec-fetch-mode': 'navigate',
    #         'sec-fetch-site': 'same-origin',
    #         'sec-fetch-user': '?1',
    #         'upgrade-insecure-requests': '1',
    #         'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/127.0.0.0 Safari/537.36'
    #     }
        
    #     data = {
    #         'form_type': 'contact',
    #         'utf8': '?',
    #         'contact[Name]': name,
    #         'contact[email]': email,
    #         'contact[Phone number]': '',
    #         'contact[Comment]': message,
    #         'h-captcha-response': hcaptcha,
    #         # 'form_key': formkey#'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa'
    #         'form_key': 'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa'
    #     }
        
    #     try:
    #         timeout = aiohttp.ClientTimeout(total=5)
    #         async with aiohttp.ClientSession(timeout=timeout) as session:
    #             async with session.post(f'{placeholder_url}/contact', headers=headers, data=data, proxy=proxy_url, ssl=False, allow_redirects=False) as response:
    #                 status = response.status
    #                 headers = dict(response.headers)
    #                 body = await response.text()
                    
    #                 # Estimate bandwidth usage
    #                 request_size = len(str(headers)) + len(str(data))
    #                 response_size = len(str(headers)) + len(body)
    #                 self.bandwidth_tracker.update(request_size, response_size)
                    
    #                 # Emit bandwidth update
    #                 self.bandwidth_update.emit(self.bandwidth_tracker.get_usage())

    #         return status, headers, body, proxy_url
    #     except aiohttp.ClientError as e:
    #         return 0, {}, "", proxy_url


    def gen_c(self):
        def generate_uuid():
            return str(uuid.uuid4())
        def generate_timestamp():
            return datetime.now().strftime("%Y-%m-%dT%H%%3A%M%%3A%S.%f")[:-3] + "Z"
        def generate_tracking_consent():
            consent = {
                "con": {"CMP": {"a": "", "m": "", "p": "", "s": ""}},
                "v": "2.1",
                "region": "USCA",
                "reg": ""
            }
            return urllib.parse.quote(json.dumps(consent, separators=(',', ':')))
        def generate_cmp_a():
            cmp_a = {
                "purposes": {"a": True, "p": True, "m": True, "t": True},
                "display_banner": False,
                "sale_of_data_region": True
            }
            return urllib.parse.quote(json.dumps(cmp_a, separators=(',', ':')))
        def generate_ga_id():
            return f"GA1.2.{random.randint(1000000000, 9999999999)}.{int(datetime.now().timestamp())}"
        current_time = datetime.now()
        timestamp = int(current_time.timestamp())
        cookies = [
            f"keep_alive={generate_uuid()}",
            "secure_customer_sig=",
            "localization=US",
            "cart_currency=USD",
            f"*tracking*consent={generate_tracking_consent()}",
            f"*cmp*a={generate_cmp_a()}",
            f"*shopify*y={generate_uuid()}",
            "*orig*referrer=",
            "*landing*page=%2Fpages%2Fcontact",
            "receive-cookie-deprecation=1",
            f"*shopify*s={generate_uuid()}",
            f"*shopify*sa_t={generate_timestamp()}",
            "*shopify*sa_p=",
            "shopify_pay_redirect=pending",
            f"*gid={generate_ga_id()}",
            "*gat=1",
            f"*ga*XSJ15S0QE1=GS1.1.{timestamp}.1.0.{timestamp}.0.0.0",
            f"*ga={generate_ga_id()}",
            "*gat_gtag_UA_196873582_1=1",
            f"*fbp=fb.1.{timestamp}593.{random.randint(100000000000000000, 999999999999999999)}",
            f"*ga_0FM3WXK1W4=GS1.2.{timestamp}.1.0.{timestamp}.0.0.0",
            "locale_bar_accepted=1",
            f"mailerlite:forms:shown:113009796894951003={random.randint(600000, 699999)}"
        ]
        return "; ".join(cookies)
    async def make_request(self, placeholder_url, hcaptcha, formkey, name, email, message):
        ips = open('proxies.txt', 'r+').read()
        ips = [ip.strip() for ip in ips.split('\n') if ip.strip()]
        proxy = random.choice(ips).split(':')
        if len(proxy) == 4:
            proxy_url = f"http://{proxy[2]}:{proxy[3]}@{proxy[0]}:{proxy[1]}"
        elif len(proxy) == 2:
            proxy_url = f"http://{proxy[0]}:{proxy[1]}"
        else:
            self.status_update.emit("ERROR", f"Invalid proxy format: {':'.join(proxy)}")
            return 502, {}, "", ""

        
        cookie = self.gen_c()
        headers ={
            "accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7",
            "accept-language": "en-US,en;q=0.9",
            "cache-control": "max-age=0",
            "content-type": "application/x-www-form-urlencoded",
            "cookie": cookie,
            "origin": "https://www.aseeminglybenigndomain.com",
            "priority": "u=0, i",
            "referer": "https://www.aseeminglybenigndomain.com/pages/contact",
            "sec-ch-ua": "\"Google Chrome\";v=\"129\", \"Not=A?Brand\";v=\"8\", \"Chromium\";v=\"129\"",
            "sec-ch-ua-mobile": "?0",
            "sec-ch-ua-platform": "\"Windows\"",
            "sec-fetch-dest": "document",
            "sec-fetch-mode": "navigate",
            "sec-fetch-site": "same-origin",
            "sec-fetch-user": "?1",
            "upgrade-insecure-requests": "1",
            "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/129.0.0.0 Safari/537.36"
        }


       
        data = {
            'form_type': 'contact',
            'utf8': '✓',
            'contact[Name]': name,
            'contact[email]': email,
            'contact[Phone number]': '',
            'contact[Comment]': message,
            'h-captcha-response': hcaptcha,
            'form_key': 'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa'
        }

        proxy_url = "http://username:password@proxyhost.com:20000"
        try:
            with httpx.Client(proxies=proxy_url) as client:
                response = client.post(f'{placeholder_url}/contact', headers=headers, data=data)
                status = response.status_code
                headers = dict(response.headers)
                body = response.text

                # Estimate bandwidth usage
                request_size = len(str(headers)) + len(str(data))
                response_size = len(str(headers)) + len(body)
                self.bandwidth_tracker.update(request_size, response_size)

                # Emit bandwidth update
                self.bandwidth_update.emit(self.bandwidth_tracker.get_usage())

            return status, headers, body, proxy_url
        except httpx.ProxyError as e:
            self.status_update.emit("ERROR", f"Proxy error: {str(e)}")
            return 0, {}, "", proxy_url
        except httpx.RequestError as e:
            self.status_update.emit("ERROR", f"Request error: {str(e)}")
            return 0, {}, "", proxy_url
        except Exception as e:
            self.status_update.emit("ERROR", f"Unexpected error: {str(e)}")
            return 0, {}, "", proxy_url

    
    async def generate_message(self, first_name, filename, placeholder_url):
        generator = MessageGenerator()
        try:
            return await generator.generate_message(first_name, placeholder_url)
        except Exception as e:
            self.status_update.emit("ERROR", f"Error generating message: {str(e)}")
            return ""

    def generate_random_string(self, length=None):
        if not length:
            length = random.randrange(5,8)
        return ''.join(random.choices('abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789', k=length))

    def generate_email(self, first_name, last_name):
        return f"{first_name.lower()}{last_name.lower()}{self.generate_random_string(4)}@useakira.com"
        #return f"{first_name.lower()}{last_name.lower()}{self.generate_random_string(4)}@d.d"

    def select_template(self, filename):
        while True:
            try:
                templates = json.load(open('templates.json'))
                break
            except Exception as e:
                print(f"Error reading templates file: {str(e)}")
                time.sleep(1)
        if filename.startswith('urls1'):
            return templates.get('template1', templates['template1'])
        elif filename.startswith('urls2'):
            return templates.get('template2', templates['template2'])
        elif filename.startswith('urls3'):
            return templates.get('template3', templates['template3'])
        return templates['template1']  # Default template

    def write_to_csv(self, timestamp, url, filename, status, message):
        with open('submissions.csv', 'a', newline='', encoding='utf-8') as csvfile:
            writer = csv.writer(csvfile)
            writer.writerow([timestamp, url, filename, status, message])

    def update_speed(self):
        current_time = datetime.now()
        one_minute_ago = current_time - timedelta(minutes=1)
        
        recent_submissions = [s for s in self.submissions_performed if s > one_minute_ago]
        
        if len(recent_submissions) == 0:
            current_speed = 0
        else:
            current_speed = len(recent_submissions)  # URLs per minute

        self.speed_update.emit(current_speed)

    def update_success_rate(self):
        total_attempts = self.successful_submissions + self.captcha_required
        if total_attempts == 0:
            success_rate = 0
        else:
            success_rate = (self.successful_submissions / total_attempts) * 100
        self.success_rate_update.emit(success_rate)

    def stop(self):
        self.stop_event.set()
        self.is_running = False
        if self.stats_task:
            self.stats_task.cancel()
    def update_json_stats(self):
        current_time = datetime.now()
        one_minute_ago = current_time - timedelta(minutes=1)
        
        tokens_last_minute = sum(1 for t in self.tokens_received if t > one_minute_ago)
        submissions_last_minute = sum(1 for s in self.submissions_performed if s > one_minute_ago)

        stats = {
            "timestamp": current_time.isoformat(),
            "processed_urls": self.processed_urls,
            "total_urls": self.total_urls,
            "successful_submissions": self.successful_submissions,
            "captcha_required": self.captcha_required,
            "pages_not_found": self.pages_not_found,
            "unknown_cases": self.unknown_cases,
            "current_speed": self.calculate_current_speed(),
            "success_rate": self.calculate_success_rate(),
            "active_threads": self.thread_count,
            "urls_in_queue": self.url_queue.qsize(),
            "tokens_received_last_minute": tokens_last_minute,
            "submissions_performed_last_minute": submissions_last_minute
        }
        
        with open('stats.json', 'w') as f:
            json.dump(stats, f, indent=2)

    def calculate_current_speed(self):
        current_time = datetime.now()
        one_minute_ago = current_time - timedelta(minutes=1)
        
        recent_submissions = [s for s in self.submission_history if s[0] > one_minute_ago]
        
        if len(recent_submissions) < 2:
            return 0
        
        oldest_time, oldest_count = recent_submissions[0]
        newest_time, newest_count = recent_submissions[-1]
        
        time_diff = (newest_time - oldest_time).total_seconds() / 60  # Convert to minutes
        url_diff = newest_count - oldest_count
        
        if time_diff > 0:
            return url_diff / time_diff
        else:
            return 0

    def calculate_success_rate(self):
        total_attempts = self.successful_submissions + self.captcha_required
        if total_attempts == 0:
            return 0
        return (self.successful_submissions / total_attempts) * 100

class MainWindow(QMainWindow):
    def __init__(self):
        super().__init__()
        self.setWindowTitle("URL Processor")
        self.setGeometry(100, 100, 800, 600)

        self.setWindowFlags(self.windowFlags() | Qt.WindowType.WindowStaysOnTopHint)
        self.central_widget = QWidget()
        self.setCentralWidget(self.central_widget)
        self.main_layout = QVBoxLayout(self.central_widget)

        self.setup_bandwidth_ui()
        self.url_lists = []
        self.setup_ui()

        self.worker = None
        self.timer = QTimer(self)
        self.timer.timeout.connect(self.update_duration)

    def setup_bandwidth_ui(self):
        bandwidth_frame = QHBoxLayout()
        self.bandwidth_label = QLabel("Bandwidth: 0 MB (0 Mbps)")
        bandwidth_frame.addWidget(self.bandwidth_label)
        self.main_layout.addLayout(bandwidth_frame)

    def setup_ui(self):
        # Top frame
        top_frame = QHBoxLayout()
        self.duration_minutes = QSpinBox()
        self.duration_minutes.setRange(0, 1000000)
        top_frame.addWidget(QLabel("Duration (min):"))
        top_frame.addWidget(self.duration_minutes)
        top_frame.addWidget(QPushButton("Set", clicked=self.update_urls_based_on_duration))
        top_frame.addWidget(QPushButton("Add URL List", clicked=self.add_url_list))
        self.main_layout.addLayout(top_frame)

        # URL Lists frame
        self.url_lists_scroll = QScrollArea()
        self.url_lists_scroll.setWidgetResizable(True)
        self.url_lists_content = QWidget()
        self.url_lists_layout = QVBoxLayout(self.url_lists_content)
        self.url_lists_scroll.setWidget(self.url_lists_content)
        self.main_layout.addWidget(self.url_lists_scroll)

        # Proxy frame
        proxy_frame = QHBoxLayout()
        self.proxy_entry = QLineEdit()
        proxy_frame.addWidget(QLabel("Proxy:"))
        proxy_frame.addWidget(self.proxy_entry)
        proxy_frame.addWidget(QPushButton("Save", clicked=self.save_proxy))
        self.main_layout.addLayout(proxy_frame)

        # Thread frame
        thread_frame = QHBoxLayout()
        self.thread_count = QSpinBox()
        self.thread_count.setRange(1, 1000)
        self.thread_count.setValue(10)  # Default value
        thread_frame.addWidget(QLabel("Thread Count:"))
        thread_frame.addWidget(self.thread_count)
        thread_frame.addWidget(QPushButton("Update", clicked=self.update_thread_count))
        self.main_layout.addLayout(thread_frame)

        # Progress frame
        progress_frame = QHBoxLayout()
        self.progress_bar = QProgressBar()
        self.progress_label = QLabel("0% (0/0)")
        progress_frame.addWidget(self.progress_bar)
        progress_frame.addWidget(self.progress_label)
        self.main_layout.addLayout(progress_frame)

        # Speed frame
        speed_frame = QHBoxLayout()
        self.speed_label = QLabel("Speed: 0 URLs/min")
        self.duration_label = QLabel("Duration: 0:00:00")
        self.estimated_label = QLabel("Estimated: 0:00:00")
        speed_frame.addWidget(self.speed_label)
        speed_frame.addWidget(self.duration_label)
        speed_frame.addWidget(self.estimated_label)
        self.main_layout.addLayout(speed_frame)

        # Success rate frame
        success_rate_frame = QHBoxLayout()
        self.success_rate_bar = QProgressBar()
        self.success_rate_label = QLabel("Success: 0%")
        success_rate_frame.addWidget(self.success_rate_bar)
        success_rate_frame.addWidget(self.success_rate_label)
        self.main_layout.addLayout(success_rate_frame)

        # Stats frame
        stats_frame = QHBoxLayout()
        self.captcha_label = QLabel("Captcha: 0")
        self.success_label = QLabel("Success: 0")
        self.unknown_label = QLabel("Pages Not Found: 0 | Unknown: 0")
        stats_frame.addWidget(self.captcha_label)
        stats_frame.addWidget(self.success_label)
        stats_frame.addWidget(self.unknown_label)
        self.main_layout.addLayout(stats_frame)

        # Thread stats frame
        thread_stats_frame = QHBoxLayout()
        self.active_threads_label = QLabel("Active Threads: 0")
        self.urls_processed_label = QLabel("URLs Being Processed: 0")
        thread_stats_frame.addWidget(self.active_threads_label)
        thread_stats_frame.addWidget(self.urls_processed_label)
        self.main_layout.addLayout(thread_stats_frame)

        # Button frame
        button_frame = QHBoxLayout()
        self.start_button = QPushButton("Start", clicked=self.start_bot)
        self.stop_button = QPushButton("Stop", clicked=self.stop_bot)
        self.stop_button.setEnabled(False)
        button_frame.addWidget(self.start_button)
        button_frame.addWidget(self.stop_button)
        self.main_layout.addLayout(button_frame)

        # Log area
        self.log_area = QTextEdit()
        self.log_area.setReadOnly(True)
        self.main_layout.addWidget(self.log_area)

    def update_urls_based_on_duration(self):
        duration = self.duration_minutes.value()
        if duration <= 0:
            self.log("ERROR", "Invalid duration. Please enter a valid time in minutes.")
            return

        total_urls = sum(url_list.length for url_list in self.url_lists)
        required_urls_per_minute = math.ceil(total_urls / duration)
        self.log("INFO", f"Required URLs per minute: {required_urls_per_minute} based on {duration} minutes for {total_urls} URLs.")

        num_lists = len(self.url_lists)
        base_urls_per_list = total_urls // num_lists
        remainder = total_urls % num_lists

        urls_distribution = [base_urls_per_list] * num_lists
        for i in range(remainder):
            urls_distribution[i] += 1

        total_urls_per_minute = 0
        for i, urls in enumerate(urls_distribution):
            urls_per_minute = math.ceil(urls / duration)
            self.url_lists[i].urls_per_minute = urls_per_minute
            self.log("INFO", f"List {i+1}: URLs per minute set to: {urls_per_minute}")
            total_urls_per_minute += urls_per_minute

        self.log("INFO", f"Total URLs assigned across all lists: {total_urls}")
        self.log("INFO", f"Total URLs per minute updated to: {total_urls_per_minute}")

        total_processed = total_urls_per_minute * duration
        if total_processed < total_urls:
            remaining = total_urls - total_processed
            self.log("WARNING", f"{remaining} URLs may not be processed within the specified duration.")
        elif total_processed > total_urls:
            extra = total_processed - total_urls
            self.log("INFO", f"All URLs will be processed. {extra} slots will be unused.")

    def add_url_list(self):
        file_path, _ = QFileDialog.getOpenFileName(self, "Select URL List File", "", "Text Files (*.txt)")
        if file_path:
            urls = self.load_urls_from_file(file_path)
            url_list = URLList(os.path.basename(file_path), urls)
            self.url_lists.append(url_list)
            self.create_url_list_widgets(url_list)

    def create_url_list_widgets(self, url_list):
        frame = QFrame()
        layout = QHBoxLayout(frame)
        layout.addWidget(QLabel(f"File: {url_list.filename}"))
        layout.addWidget(QLabel(f"URLs: {url_list.length}"))
        layout.addWidget(QLabel("URLs/min:"))
        urls_per_minute = QSpinBox()
        urls_per_minute.setRange(0, 1000000)
        urls_per_minute.setValue(url_list.urls_per_minute)
        layout.addWidget(urls_per_minute)
        update_button = QPushButton("Update")
        update_button.clicked.connect(lambda: self.update_url_list_rate(url_list, urls_per_minute.value()))
        layout.addWidget(update_button)
        self.url_lists_layout.addWidget(frame)

    def load_urls_from_file(self, file_path):
        try:
            with open(file_path, 'r') as file:
                urls = file.read().splitlines()
                urls = [url for url in urls if url]
            self.log("INFO", f"Loaded {len(urls)} URLs from {os.path.basename(file_path)}")
            return urls
        except Exception as e:
            self.log("ERROR", f"Error loading URLs from file: {str(e)}")
            return []

    def update_url_list_rate(self, url_list, new_rate):
        url_list.urls_per_minute = new_rate
        self.log("INFO", f"Updated {url_list.filename} to {new_rate} URLs/min")

    def save_proxy(self):
        proxy = self.proxy_entry.text()
        self.log("INFO", f"Proxy saved: {proxy}")

    def update_thread_count(self):
        new_count = self.thread_count.value()
        if new_count > 0:
            self.log("INFO", f"Thread count updated to {new_count}")
            if self.worker and self.worker.is_running:
                self.stop_bot()
                self.start_bot()
        else:
            self.log("ERROR", "Invalid thread count. Please enter a positive integer.")

    def update_bandwidth(self, usage):
        self.bandwidth_label.setText(f"Bandwidth: {usage['total_mb']:.2f} MB ({usage['mbps']:.2f} Mbps)")

    def start_bot(self):
        if not self.url_lists:
            self.log("ERROR", "No URL lists loaded. Please add at least one URL list.")
            return

        self.worker = AsyncWorker(self.url_lists, self.proxy_entry.text(), self.thread_count.value())
        self.worker.progress_update.connect(self.update_progress)
        self.worker.status_update.connect(self.log)
        self.worker.speed_update.connect(self.update_speed)
        self.worker.success_rate_update.connect(self.update_success_rate)
        self.worker.stats_update.connect(self.update_statistics)
        self.worker.thread_stats_update.connect(self.update_thread_stats)
        self.worker.bandwidth_update.connect(self.update_bandwidth)
        
        for url_list in self.url_lists:
            for url in url_list.urls:
                self.worker.url_queue.put_nowait((0, (url, url_list.filename)))

        self.worker.start()
        self.timer.start(1000)  # Update every second
        
        self.start_button.setEnabled(False)
        self.stop_button.setEnabled(True)

        self.log("INFO", "Bot started.")
    
    def stop_bot(self):
        if self.worker and self.worker.is_running:
            self.worker.stop()
            self.worker.wait()
            self.timer.stop()
            self.start_button.setEnabled(True)
            self.stop_button.setEnabled(False)
            self.log("INFO", "Bot stopped.")

    def update_progress(self, processed, total, percentage):
        self.progress_bar.setValue(percentage)
        self.progress_label.setText(f"{percentage}% ({processed}/{total})")

    def update_speed(self, speed):
        self.speed_label.setText(f"Speed: {speed:.2f} URLs/min")

    def update_success_rate(self, rate):
        self.success_rate_bar.setValue(int(rate))
        self.success_rate_label.setText(f"Success: {rate:.2f}%")

    def update_statistics(self, captcha, success, not_found, unknown):
        self.captcha_label.setText(f"Captcha: {captcha}")
        self.success_label.setText(f"Success: {success}")
        self.unknown_label.setText(f"Pages Not Found: {not_found} | Unknown: {unknown}")

    def update_thread_stats(self, active_threads, urls_processed):
        self.active_threads_label.setText(f"Active Threads: {active_threads}")
        self.urls_processed_label.setText(f"URLs Being Processed: {urls_processed}")

    def update_duration(self):
        if self.worker and self.worker.start_time:
            elapsed = (datetime.now() - self.worker.start_time).total_seconds()
            self.duration_label.setText(f"Duration: {self.format_time(int(elapsed))}")
            if self.worker.processed_urls > 0:
                estimated_total = elapsed * self.worker.total_urls / self.worker.processed_urls
                self.estimated_label.setText(f"Estimated: {self.format_time(int(estimated_total))}")

    def format_time(self, seconds):
        hours, remainder = divmod(seconds, 3600)
        minutes, seconds = divmod(remainder, 60)
        return f"{int(hours):02}:{int(minutes):02}:{int(seconds):02}"

    def log(self, level, message):
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        log_message = f"[{timestamp}] [{level}] {message}"
        self.log_area.append(log_message)
        self.log_area.verticalScrollBar().setValue(self.log_area.verticalScrollBar().maximum())

if __name__ == "__main__":
    app = QApplication(sys.argv)
    window = MainWindow()
    window.show()
    sys.exit(app.exec())